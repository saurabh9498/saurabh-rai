# RAG (Retrieval-Augmented Generation) Pipeline Configuration
# This file configures the document processing, embedding, retrieval, and generation components.

# =============================================================================
# Document Processing
# =============================================================================
document_processing:
  # Supported file types for ingestion
  supported_formats:
    - ".pdf"
    - ".docx"
    - ".doc"
    - ".txt"
    - ".md"
    - ".html"
    - ".json"
    - ".csv"
  
  # Text extraction settings
  extraction:
    pdf_parser: "pypdf2"  # Options: pypdf2, pdfplumber, unstructured
    ocr_enabled: true
    ocr_language: "eng"
    preserve_formatting: false
    extract_tables: true
    extract_images: false

# =============================================================================
# Chunking Strategy
# =============================================================================
chunking:
  # Primary chunking method
  strategy: "semantic"  # Options: fixed, semantic, recursive, sentence
  
  # Fixed-size chunking parameters
  fixed:
    chunk_size: 512
    chunk_overlap: 50
  
  # Semantic chunking parameters
  semantic:
    max_chunk_size: 1024
    min_chunk_size: 100
    similarity_threshold: 0.85
    breakpoint_threshold_type: "percentile"  # percentile, standard_deviation, interquartile
    breakpoint_threshold_amount: 95
  
  # Recursive chunking parameters
  recursive:
    chunk_size: 512
    chunk_overlap: 50
    separators:
      - "\n\n"
      - "\n"
      - ". "
      - " "
      - ""
  
  # Sentence-based chunking
  sentence:
    sentences_per_chunk: 5
    overlap_sentences: 1

# =============================================================================
# Embedding Configuration
# =============================================================================
embeddings:
  # Primary embedding model
  provider: "openai"  # Options: openai, sentence_transformers, cohere, huggingface
  
  # OpenAI embeddings
  openai:
    model: "text-embedding-3-small"
    dimensions: 1536
    batch_size: 100
  
  # Sentence Transformers (local)
  sentence_transformers:
    model: "all-MiniLM-L6-v2"
    device: "cuda"  # Options: cuda, cpu, mps
    normalize_embeddings: true
  
  # Cohere embeddings
  cohere:
    model: "embed-english-v3.0"
    input_type: "search_document"  # search_document, search_query
  
  # Caching
  cache:
    enabled: true
    backend: "redis"  # Options: redis, sqlite, memory
    ttl_seconds: 86400  # 24 hours

# =============================================================================
# Vector Store Configuration
# =============================================================================
vector_store:
  # Vector database provider
  provider: "chroma"  # Options: chroma, pinecone, weaviate, qdrant, milvus
  
  # ChromaDB settings
  chroma:
    persist_directory: "./data/chroma_db"
    collection_name: "knowledge_base"
    distance_metric: "cosine"  # Options: cosine, l2, ip
  
  # Pinecone settings
  pinecone:
    index_name: "multi-agent-rag"
    environment: "us-west1-gcp"
    metric: "cosine"
    pod_type: "p1.x1"
  
  # Qdrant settings
  qdrant:
    url: "http://localhost:6333"
    collection_name: "knowledge_base"
    vector_size: 1536
    distance: "Cosine"
  
  # Index optimization
  indexing:
    batch_size: 500
    parallel_workers: 4

# =============================================================================
# Retrieval Configuration
# =============================================================================
retrieval:
  # Search strategy
  strategy: "hybrid"  # Options: dense, sparse, hybrid
  
  # Dense retrieval (vector similarity)
  dense:
    top_k: 10
    score_threshold: 0.7
  
  # Sparse retrieval (BM25)
  sparse:
    top_k: 10
    b: 0.75
    k1: 1.2
  
  # Hybrid search (combines dense + sparse)
  hybrid:
    dense_weight: 0.7
    sparse_weight: 0.3
    top_k: 10
    fusion_method: "rrf"  # Options: rrf (reciprocal rank fusion), linear
  
  # Query expansion
  query_expansion:
    enabled: true
    method: "llm"  # Options: llm, synonyms, none
    num_expansions: 3
  
  # Metadata filtering
  filtering:
    enabled: true
    default_filters: {}

# =============================================================================
# Reranking Configuration
# =============================================================================
reranking:
  enabled: true
  
  # Reranker model
  provider: "cross_encoder"  # Options: cross_encoder, cohere, llm
  
  # Cross-encoder reranking
  cross_encoder:
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    device: "cuda"
    batch_size: 32
  
  # Cohere reranking
  cohere:
    model: "rerank-english-v2.0"
    top_n: 5
  
  # LLM-based reranking
  llm:
    model: "gpt-4o-mini"
    prompt_template: "rerank_prompt"
  
  # Final results
  top_k: 5
  score_threshold: 0.5

# =============================================================================
# Generation Configuration
# =============================================================================
generation:
  # LLM for response generation
  provider: "openai"
  
  # Model settings
  model: "gpt-4o"
  temperature: 0.7
  max_tokens: 2048
  
  # Context handling
  context:
    max_context_length: 8000
    context_strategy: "stuff"  # Options: stuff, map_reduce, refine
    include_metadata: true
  
  # Prompt templates
  prompts:
    system: "rag_system_prompt"
    query: "rag_query_prompt"
  
  # Citation settings
  citations:
    enabled: true
    format: "inline"  # Options: inline, footnote, none
    include_scores: false

# =============================================================================
# Evaluation & Monitoring
# =============================================================================
evaluation:
  # Metrics to track
  metrics:
    - "answer_relevancy"
    - "faithfulness"
    - "context_precision"
    - "context_recall"
  
  # Evaluation framework
  framework: "ragas"  # Options: ragas, custom
  
  # Logging
  logging:
    enabled: true
    log_queries: true
    log_retrievals: true
    log_generations: true
    storage: "file"  # Options: file, database

# =============================================================================
# Performance Optimization
# =============================================================================
performance:
  # Caching
  cache:
    query_cache: true
    query_cache_ttl: 3600
    embedding_cache: true
  
  # Async processing
  async:
    enabled: true
    max_concurrent_requests: 10
  
  # Batching
  batching:
    enabled: true
    batch_size: 10
    batch_timeout_ms: 100
