{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent AI System - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the core capabilities of the Multi-Agent AI System:\n",
    "1. Document ingestion and RAG queries\n",
    "2. Multi-agent orchestration\n",
    "3. Specialized agent tasks (research, analysis, code)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the RAG Pipeline\n",
    "\n",
    "First, let's set up the RAG pipeline for document ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag import RAGPipeline, Document, ChunkingStrategy\n",
    "from src.rag.embeddings import get_embedding_model\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = get_embedding_model(provider=\"openai\")\n",
    "print(f\"üìä Embedding model: {embedding_model.model_name}\")\n",
    "print(f\"   Dimension: {embedding_model.dimension}\")\n",
    "\n",
    "# Initialize RAG pipeline\n",
    "rag = RAGPipeline(\n",
    "    embedding_model=embedding_model,\n",
    "    collection_name=\"demo_collection\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ RAG Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for demonstration\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        Multi-Agent AI Systems represent a paradigm shift in how we build intelligent applications.\n",
    "        Unlike monolithic AI systems, multi-agent architectures decompose complex tasks into \n",
    "        specialized subtasks handled by different agents. Each agent has specific capabilities\n",
    "        and can communicate with other agents to solve problems collaboratively.\n",
    "        \n",
    "        Key benefits include:\n",
    "        - Modularity: Each agent can be developed and tested independently\n",
    "        - Scalability: New agents can be added without affecting existing ones\n",
    "        - Specialization: Agents can be optimized for specific task types\n",
    "        - Robustness: System continues functioning if one agent fails\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"architecture_guide\", \"topic\": \"multi-agent systems\"}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        RAG (Retrieval-Augmented Generation) combines the power of retrieval systems with\n",
    "        large language models. Instead of relying solely on parametric knowledge, RAG systems\n",
    "        retrieve relevant documents from a knowledge base and use them to ground responses.\n",
    "        \n",
    "        The typical RAG pipeline includes:\n",
    "        1. Document chunking: Breaking documents into manageable pieces\n",
    "        2. Embedding: Converting text to vector representations\n",
    "        3. Indexing: Storing vectors in a searchable database\n",
    "        4. Retrieval: Finding relevant chunks for a query\n",
    "        5. Generation: Using retrieved context to generate responses\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"rag_documentation\", \"topic\": \"RAG\"}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"\n",
    "        Best practices for prompt engineering with multi-agent systems:\n",
    "        \n",
    "        1. Clear Role Definition: Each agent should have a well-defined role and capabilities\n",
    "        2. Structured Outputs: Use JSON or structured formats for inter-agent communication\n",
    "        3. Error Handling: Agents should gracefully handle failures and provide feedback\n",
    "        4. Context Management: Carefully manage context to stay within token limits\n",
    "        5. Tracing: Implement comprehensive logging for debugging and optimization\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"best_practices\", \"topic\": \"prompt engineering\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Ingest documents\n",
    "import asyncio\n",
    "\n",
    "async def ingest():\n",
    "    result = await rag.ingest_documents(\n",
    "        documents=sample_documents,\n",
    "        chunking_strategy=ChunkingStrategy.RECURSIVE\n",
    "    )\n",
    "    return result\n",
    "\n",
    "ingestion_result = asyncio.get_event_loop().run_until_complete(ingest())\n",
    "print(f\"üìö Ingested documents:\")\n",
    "print(f\"   Documents: {ingestion_result['documents_processed']}\")\n",
    "print(f\"   Chunks: {ingestion_result['chunks_created']}\")\n",
    "print(f\"   Embeddings: {ingestion_result['embeddings_generated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search(query: str):\n",
    "    result = await rag.query(query, top_k=3)\n",
    "    return result\n",
    "\n",
    "# Example query\n",
    "query = \"What are the key benefits of multi-agent systems?\"\n",
    "query_result = asyncio.get_event_loop().run_until_complete(search(query))\n",
    "\n",
    "print(f\"üîç Query: {query}\\n\")\n",
    "print(\"üìÑ Retrieved Documents:\")\n",
    "for i, (doc, score) in enumerate(zip(query_result.documents, query_result.scores)):\n",
    "    print(f\"\\n--- Result {i+1} (Score: {score:.3f}) ---\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"Content: {doc.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Multi-Agent System\n",
    "\n",
    "Now let's set up the multi-agent orchestrator with specialized agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents import OrchestratorAgent, ResearchAgent, AnalystAgent, CodeAgent\n",
    "from src.core import get_llm, ConversationContext\n",
    "\n",
    "# Initialize LLM\n",
    "llm = get_llm(provider=\"openai\", model=\"gpt-4-turbo\")\n",
    "\n",
    "# Initialize specialized agents\n",
    "research_agent = ResearchAgent(llm=llm, rag_pipeline=rag)\n",
    "analyst_agent = AnalystAgent(llm=llm)\n",
    "code_agent = CodeAgent(llm=llm, enable_execution=True)\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = OrchestratorAgent(\n",
    "    llm=llm,\n",
    "    agents={\n",
    "        \"research\": research_agent,\n",
    "        \"analyst\": analyst_agent,\n",
    "        \"code\": code_agent\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Multi-Agent System initialized!\")\n",
    "print(f\"   Available agents: {list(orchestrator.agents.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute Multi-Agent Workflows\n",
    "\n",
    "### Example 1: Research Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research task\n",
    "research_task = \"Explain the benefits of RAG in multi-agent systems and how they work together\"\n",
    "\n",
    "async def run_research():\n",
    "    context = ConversationContext()\n",
    "    result = await orchestrator.execute_workflow(research_task, context)\n",
    "    return result\n",
    "\n",
    "research_result = asyncio.get_event_loop().run_until_complete(run_research())\n",
    "\n",
    "print(\"üî¨ Research Task Result:\")\n",
    "print(f\"   Success: {research_result.success}\")\n",
    "print(f\"   Execution Time: {research_result.total_time:.2f}s\")\n",
    "print(f\"   Agents Used: {research_result.execution_order}\")\n",
    "print(f\"\\nüìù Response:\\n{research_result.final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Code Generation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct code agent usage\n",
    "code_task = \"Write a Python function that implements exponential backoff retry logic\"\n",
    "\n",
    "async def run_code_task():\n",
    "    context = ConversationContext()\n",
    "    result = await code_agent.execute(code_task, context)\n",
    "    return result\n",
    "\n",
    "code_result = asyncio.get_event_loop().run_until_complete(run_code_task())\n",
    "\n",
    "print(\"üíª Code Generation Result:\")\n",
    "print(f\"   Success: {code_result.success}\")\n",
    "print(f\"   Execution Time: {code_result.execution_time:.2f}s\")\n",
    "if code_result.success and code_result.output:\n",
    "    print(f\"\\nüìù Generated Code:\")\n",
    "    print(code_result.output.get('code', 'No code generated'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Analysis Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for analysis\n",
    "sample_data = {\n",
    "    \"monthly_users\": [1000, 1200, 1500, 1400, 1800, 2200, 2500],\n",
    "    \"conversion_rate\": [0.02, 0.025, 0.022, 0.028, 0.03, 0.032, 0.035],\n",
    "    \"months\": [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\"]\n",
    "}\n",
    "\n",
    "analysis_task = \"Analyze the user growth and conversion rate trends. What recommendations do you have?\"\n",
    "\n",
    "async def run_analysis():\n",
    "    context = ConversationContext()\n",
    "    result = await analyst_agent.execute(\n",
    "        analysis_task, \n",
    "        context,\n",
    "        data=sample_data\n",
    "    )\n",
    "    return result\n",
    "\n",
    "analysis_result = asyncio.get_event_loop().run_until_complete(run_analysis())\n",
    "\n",
    "print(\"üìä Analysis Result:\")\n",
    "print(f\"   Success: {analysis_result.success}\")\n",
    "print(f\"   Execution Time: {analysis_result.execution_time:.2f}s\")\n",
    "if analysis_result.success and analysis_result.output:\n",
    "    output = analysis_result.output\n",
    "    print(f\"\\nüìà Summary: {output.get('summary', 'N/A')}\")\n",
    "    if 'recommendations' in output:\n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        for rec in output['recommendations']:\n",
    "            print(f\"   - {rec.get('action', rec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complex Multi-Agent Workflow\n",
    "\n",
    "Let's execute a complex task that requires multiple agents to collaborate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_task = \"\"\"\n",
    "I need help building a data pipeline:\n",
    "1. First, research best practices for ETL pipelines\n",
    "2. Then analyze the trade-offs between batch and stream processing\n",
    "3. Finally, generate Python code for a simple ETL pipeline skeleton\n",
    "\"\"\"\n",
    "\n",
    "async def run_complex_workflow():\n",
    "    context = ConversationContext()\n",
    "    result = await orchestrator.execute_workflow(complex_task, context)\n",
    "    return result\n",
    "\n",
    "complex_result = asyncio.get_event_loop().run_until_complete(run_complex_workflow())\n",
    "\n",
    "print(\"üöÄ Complex Workflow Result:\")\n",
    "print(f\"   Success: {complex_result.success}\")\n",
    "print(f\"   Total Time: {complex_result.total_time:.2f}s\")\n",
    "print(f\"   Execution Order: {complex_result.execution_order}\")\n",
    "print(f\"\\nüìã Agent Results:\")\n",
    "for agent_name, result in complex_result.agent_results.items():\n",
    "    print(f\"\\n   {agent_name}:\")\n",
    "    print(f\"      Success: {result.get('success', 'N/A')}\")\n",
    "    print(f\"      Time: {result.get('execution_time', 0):.2f}s\")\n",
    "\n",
    "print(f\"\\nüìù Final Output:\\n{complex_result.final_output[:1000]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Delete the demo collection\n",
    "# rag.delete_collection()\n",
    "# print(\"üßπ Demo collection deleted\")\n",
    "\n",
    "print(\"\\n‚úÖ Demo complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Try different queries with the RAG pipeline\")\n",
    "print(\"  2. Experiment with different agent combinations\")\n",
    "print(\"  3. Ingest your own documents into the knowledge base\")\n",
    "print(\"  4. Build custom tools for your agents\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
