{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Model Analysis\n",
    "\n",
    "This notebook demonstrates the fraud detection ensemble and analyzes model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "For demonstration, we generate synthetic transaction data with realistic fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=10000, fraud_rate=0.02, random_state=42):\n",
    "    \"\"\"Generate synthetic fraud detection data.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_fraud = int(n_samples * fraud_rate)\n",
    "    n_legit = n_samples - n_fraud\n",
    "    \n",
    "    # Feature names\n",
    "    features = [\n",
    "        'txn_count_1h', 'txn_count_6h', 'txn_count_24h', 'txn_count_7d',\n",
    "        'amount_sum_1h', 'amount_sum_24h', 'amount_avg_30d', 'amount_std_30d',\n",
    "        'time_since_last_txn', 'unique_merchants_24h', 'unique_channels_24h',\n",
    "        'is_first_transaction', 'is_new_merchant', 'is_new_device',\n",
    "        'deviation_from_avg', 'merchant_risk_score', 'device_risk_score'\n",
    "    ]\n",
    "    \n",
    "    # Legitimate transactions\n",
    "    legit_data = {\n",
    "        'txn_count_1h': np.random.poisson(2, n_legit),\n",
    "        'txn_count_6h': np.random.poisson(5, n_legit),\n",
    "        'txn_count_24h': np.random.poisson(10, n_legit),\n",
    "        'txn_count_7d': np.random.poisson(30, n_legit),\n",
    "        'amount_sum_1h': np.random.exponential(100, n_legit),\n",
    "        'amount_sum_24h': np.random.exponential(500, n_legit),\n",
    "        'amount_avg_30d': np.random.normal(150, 50, n_legit),\n",
    "        'amount_std_30d': np.random.exponential(50, n_legit),\n",
    "        'time_since_last_txn': np.random.exponential(7200, n_legit),\n",
    "        'unique_merchants_24h': np.random.poisson(3, n_legit),\n",
    "        'unique_channels_24h': np.random.choice([1, 2], n_legit, p=[0.8, 0.2]),\n",
    "        'is_first_transaction': np.random.choice([0, 1], n_legit, p=[0.99, 0.01]),\n",
    "        'is_new_merchant': np.random.choice([0, 1], n_legit, p=[0.9, 0.1]),\n",
    "        'is_new_device': np.random.choice([0, 1], n_legit, p=[0.95, 0.05]),\n",
    "        'deviation_from_avg': np.random.normal(0, 1, n_legit),\n",
    "        'merchant_risk_score': np.random.beta(2, 10, n_legit),\n",
    "        'device_risk_score': np.random.beta(2, 10, n_legit),\n",
    "    }\n",
    "    \n",
    "    # Fraudulent transactions (different patterns)\n",
    "    fraud_data = {\n",
    "        'txn_count_1h': np.random.poisson(8, n_fraud),\n",
    "        'txn_count_6h': np.random.poisson(15, n_fraud),\n",
    "        'txn_count_24h': np.random.poisson(25, n_fraud),\n",
    "        'txn_count_7d': np.random.poisson(40, n_fraud),\n",
    "        'amount_sum_1h': np.random.exponential(500, n_fraud),\n",
    "        'amount_sum_24h': np.random.exponential(2000, n_fraud),\n",
    "        'amount_avg_30d': np.random.normal(150, 50, n_fraud),\n",
    "        'amount_std_30d': np.random.exponential(50, n_fraud),\n",
    "        'time_since_last_txn': np.random.exponential(600, n_fraud),\n",
    "        'unique_merchants_24h': np.random.poisson(8, n_fraud),\n",
    "        'unique_channels_24h': np.random.choice([1, 2, 3], n_fraud, p=[0.3, 0.4, 0.3]),\n",
    "        'is_first_transaction': np.random.choice([0, 1], n_fraud, p=[0.7, 0.3]),\n",
    "        'is_new_merchant': np.random.choice([0, 1], n_fraud, p=[0.4, 0.6]),\n",
    "        'is_new_device': np.random.choice([0, 1], n_fraud, p=[0.5, 0.5]),\n",
    "        'deviation_from_avg': np.random.normal(3, 2, n_fraud),\n",
    "        'merchant_risk_score': np.random.beta(5, 5, n_fraud),\n",
    "        'device_risk_score': np.random.beta(5, 5, n_fraud),\n",
    "    }\n",
    "    \n",
    "    # Combine\n",
    "    df_legit = pd.DataFrame(legit_data)\n",
    "    df_legit['is_fraud'] = 0\n",
    "    \n",
    "    df_fraud = pd.DataFrame(fraud_data)\n",
    "    df_fraud['is_fraud'] = 1\n",
    "    \n",
    "    df = pd.concat([df_legit, df_fraud], ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_synthetic_data(n_samples=10000, fraud_rate=0.02)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean():.2%}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by fraud label\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "features_to_plot = [\n",
    "    'txn_count_1h', 'amount_sum_1h', 'time_since_last_txn',\n",
    "    'unique_merchants_24h', 'deviation_from_avg', 'merchant_risk_score',\n",
    "    'is_new_merchant', 'is_new_device', 'device_risk_score'\n",
    "]\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    for label, color in [(0, 'green'), (1, 'red')]:\n",
    "        data = df[df['is_fraud'] == label][feature]\n",
    "        ax.hist(data, bins=30, alpha=0.5, label=f'Fraud={label}', color=color, density=True)\n",
    "    \n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Feature Distributions by Fraud Label', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "# Split data\n",
    "feature_cols = [c for c in df.columns if c != 'is_fraud']\n",
    "X = df[feature_cols].values\n",
    "y = df['is_fraud'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"Train fraud rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test fraud rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest (simulating XGBoost)\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_model = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.02,\n",
    "    random_state=42\n",
    ")\n",
    "iso_model.fit(X_train)\n",
    "\n",
    "print('Models trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "rf_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "iso_scores = -iso_model.decision_function(X_test)  # Negate so higher = more anomalous\n",
    "iso_proba = 1 / (1 + np.exp(-5 * (iso_scores - 0.5)))  # Convert to probability\n",
    "\n",
    "# Ensemble (weighted average)\n",
    "ensemble_proba = 0.7 * rf_proba + 0.3 * iso_proba\n",
    "y_pred = (ensemble_proba >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "print(f\"\\nAUC-ROC: {roc_auc_score(y_test, ensemble_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Legit', 'Fraud'], yticklabels=['Legit', 'Fraud'])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, ensemble_proba)\n",
    "auc = roc_auc_score(y_test, ensemble_proba)\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'Ensemble (AUC={auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, ensemble_proba)\n",
    "axes[2].plot(recall, precision, 'g-', linewidth=2)\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Precision-Recall Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(importance['feature'], importance['importance'], color='steelblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Features:\")\n",
    "print(importance.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Score Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score distribution by label\n",
    "for label, color, name in [(0, 'green', 'Legitimate'), (1, 'red', 'Fraud')]:\n",
    "    scores = ensemble_proba[y_test == label]\n",
    "    axes[0].hist(scores, bins=50, alpha=0.6, label=name, color=color, density=True)\n",
    "    \n",
    "axes[0].axvline(x=0.3, color='orange', linestyle='--', label='Approve Threshold')\n",
    "axes[0].axvline(x=0.7, color='yellow', linestyle='--', label='Review Threshold')\n",
    "axes[0].axvline(x=0.9, color='red', linestyle='--', label='Decline Threshold')\n",
    "axes[0].set_xlabel('Risk Score')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Risk Score Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Decision breakdown\n",
    "decisions = []\n",
    "for score in ensemble_proba:\n",
    "    if score < 0.3:\n",
    "        decisions.append('Approve')\n",
    "    elif score < 0.7:\n",
    "        decisions.append('Step-up')\n",
    "    elif score < 0.9:\n",
    "        decisions.append('Review')\n",
    "    else:\n",
    "        decisions.append('Decline')\n",
    "\n",
    "decision_counts = pd.Series(decisions).value_counts()\n",
    "colors = ['green', 'orange', 'yellow', 'red']\n",
    "axes[1].pie(decision_counts, labels=decision_counts.index, autopct='%1.1f%%', colors=colors[:len(decision_counts)])\n",
    "axes[1].set_title('Decision Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Synthetic fraud data generation with realistic patterns\n",
    "- Ensemble model training (RF + Isolation Forest)\n",
    "- Performance evaluation (AUC, Precision, Recall)\n",
    "- Feature importance analysis\n",
    "- Score distribution and decision breakdown\n",
    "\n",
    "The production system uses XGBoost + Neural Network + Isolation Forest for even better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
