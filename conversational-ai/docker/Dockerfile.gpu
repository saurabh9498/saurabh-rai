# =============================================================================
# Conversational AI Assistant - GPU-Optimized Dockerfile
# =============================================================================
# Built on NVIDIA PyTorch container with CUDA, cuDNN, and TensorRT
# Optimized for GPU-accelerated ASR (Whisper) and NLU inference
#
# Build:
#   docker build -f docker/Dockerfile.gpu -t conversational-ai:gpu .
#
# Run:
#   docker run --gpus all -p 8000:8000 conversational-ai:gpu

# -----------------------------------------------------------------------------
# Base: NVIDIA PyTorch with CUDA 12.x
# -----------------------------------------------------------------------------
FROM nvcr.io/nvidia/pytorch:23.10-py3

# Environment configuration
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    # CUDA configuration
    CUDA_VISIBLE_DEVICES=0 \
    # Force GPU usage
    USE_GPU=true \
    ASR_DEVICE=cuda \
    NLU_DEVICE=cuda \
    TTS_DEVICE=cuda \
    # Whisper settings
    WHISPER_CACHE_DIR=/app/models/whisper \
    # Performance tuning
    OMP_NUM_THREADS=4 \
    MKL_NUM_THREADS=4

WORKDIR /app

# -----------------------------------------------------------------------------
# System Dependencies
# -----------------------------------------------------------------------------
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Audio processing
    ffmpeg \
    libsndfile1 \
    libportaudio2 \
    portaudio19-dev \
    # Build tools
    build-essential \
    # Utilities
    curl \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create non-root user
RUN groupadd --gid 1000 appgroup \
    && useradd --uid 1000 --gid appgroup --shell /bin/bash --create-home appuser

# -----------------------------------------------------------------------------
# Python Dependencies
# -----------------------------------------------------------------------------
COPY requirements.txt .

# Install Python packages
RUN pip install --upgrade pip wheel setuptools \
    && pip install -r requirements.txt

# Install additional GPU-optimized packages
RUN pip install \
    # Faster Whisper (CTranslate2 optimized)
    faster-whisper>=0.10.0 \
    # TensorRT for inference optimization
    tensorrt \
    # ONNX for model export
    onnx \
    onnxruntime-gpu

# Download NLP models
RUN python -m spacy download en_core_web_sm

# -----------------------------------------------------------------------------
# Download ML Models (Optional: can be done at runtime)
# -----------------------------------------------------------------------------
ARG PRELOAD_MODELS=true
ARG WHISPER_MODEL=base

RUN if [ "$PRELOAD_MODELS" = "true" ]; then \
        echo "Downloading Whisper model: $WHISPER_MODEL" && \
        python -c "import whisper; whisper.load_model('$WHISPER_MODEL')" && \
        echo "Downloading Faster-Whisper model: $WHISPER_MODEL" && \
        python -c "from faster_whisper import WhisperModel; WhisperModel('$WHISPER_MODEL', device='cpu', compute_type='int8')" || true \
    ; fi

# -----------------------------------------------------------------------------
# Application Code
# -----------------------------------------------------------------------------
COPY --chown=appuser:appgroup src/ ./src/
COPY --chown=appuser:appgroup configs/ ./configs/
COPY --chown=appuser:appgroup scripts/ ./scripts/

# Create directories
RUN mkdir -p /app/models /app/data /app/logs \
    && chown -R appuser:appgroup /app

# -----------------------------------------------------------------------------
# Runtime Configuration
# -----------------------------------------------------------------------------
# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Expose API port
EXPOSE 8000

# Switch to non-root user
USER appuser

# Default command with GPU workers
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]

# -----------------------------------------------------------------------------
# Metadata
# -----------------------------------------------------------------------------
LABEL maintainer="your-email@example.com" \
      version="1.0.0" \
      description="GPU-optimized Conversational AI Assistant" \
      org.opencontainers.image.source="https://github.com/yourusername/conversational-ai"
